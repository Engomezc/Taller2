{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0d4e0-e979-4b6e-9ad5-36646dd3c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import select as something\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from pymongo import MongoClient\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622923d2-fb3e-4af6-873e-9b819031643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['taller2']\n",
    "collection = db['elespectador']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88616fa7-9437-4071-a1be-f69fe301c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the site to be analyzed\n",
    "SITE_URL = 'https://www.elespectador.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7f01b-82de-44d9-99a3-bad29a995995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox web driver path\n",
    "# Download the driver for you S.O. here: https://github.com/mozilla/geckodriver/releases\n",
    "DRIVER_PATH = './geckodriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d99074-b05d-4aee-a34d-eed76ed9a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new firefox window\n",
    "browser = webdriver.Firefox(executable_path = DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c549d3c-148a-46f7-83af-078ce555116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(browser, relative_path):\n",
    "    # Making the request and rendering the browser\n",
    "    browser.get(SITE_URL + relative_path)\n",
    "    \n",
    "    # Simulating vertical scrolling for handling lazy load\n",
    "    check_height = browser.execute_script('return document.body.scrollHeight;')\n",
    "    while True:\n",
    "        browser.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        time.sleep(3)\n",
    "        height = browser.execute_script('return document.body.scrollHeight;')\n",
    "        if height == check_height: \n",
    "            break \n",
    "        check_height = height\n",
    "    \n",
    "    # Getting HTML content and passing it to BeautifulSoup for scraping analysis\n",
    "    return BeautifulSoup(browser.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e592-7400-44d1-82d2-6aefbb6b7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range (1,32):\n",
    "    soup = make_request(browser, '/archivo/politica/')\n",
    "    entrada = browser.find_element_by_class_name('Pagination-Input')\n",
    "    entrada.send_keys(Keys.BACK_SPACE)\n",
    "    entrada.send_keys(Keys.BACK_SPACE)\n",
    "    entrada.send_keys(x)\n",
    "    entrada.send_keys(Keys.ENTER)\n",
    "    time.sleep(15)\n",
    "    \n",
    "# Finding the section where news are contained \n",
    "    layout = soup.find(class_ = 'Layout-flexAds')\n",
    "\n",
    "# Getting blocks from layout\n",
    "    blocks = layout.find_all(class_ = 'Container Block', recursive = True)\n",
    "    print(len(blocks))\n",
    "    \n",
    "# Finding and concatenating news cards\n",
    "    cards = blocks[0].find_all(class_ = 'Card_rowCardLeft')\n",
    "    len(cards)\n",
    "\n",
    "# Building a list with title and relative path of the news founded\n",
    "    news = []\n",
    "\n",
    "    for card in cards:\n",
    "        news.append({\n",
    "        'title': card.find('h2', class_ = 'Card-Title').find('a').get_text(),\n",
    "        'relative_path': card.find('h2', class_ = 'Card-Title').find('a')['href'],       \n",
    "    })\n",
    "    \n",
    "    news\n",
    "    \n",
    "    for n in news:\n",
    "        # Getting HTML content for each news page\n",
    "        soup = make_request(browser, n['relative_path'])\n",
    "\n",
    "        # Extracting news metadata\n",
    "        n['datetime'] = soup.find(class_ = 'ArticleHeader-Date').get_text()\n",
    "        n['author'] = soup.find(class_ = 'ACredit-Author').find('a').get_text()\n",
    "        n['summary'] = soup.find(class_ = 'ArticleHeader-Hook').find('div').get_text()\n",
    "        n['category'] = soup.find(class_ = 'Breadcrumb').find('a').get_text()\n",
    "\n",
    "        # Extracting and concatenating news full text\n",
    "        paragraphs = soup.find_all(class_ = 'font--secondary')\n",
    "        n['full_text'] = ' '.join([p.get_text() for p in paragraphs])\n",
    "\n",
    "    news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345c166-567b-4e9c-8797-96f087c1aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing extracted information for further analysis\n",
    "collection.insert_many(news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
